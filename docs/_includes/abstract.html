<h1>Abstract</h1>
<p>Trajectory optimizers for model-based reinforcement learning, such as the  Cross-Entropy Method (CEM), can yield compelling results even in high-dimensional control tasks and sparse-reward environments.
However, their sampling inefficiency prevents them from being used for real-time planning and control.
We propose an improved version of the CEM algorithm for fast planning, with novel additions including temporally-correlated actions and memory,
requiring 2.7-22&times; less samples and yielding a performance increase of 1.2-10&times; in high-dimensional control problems.</p>
